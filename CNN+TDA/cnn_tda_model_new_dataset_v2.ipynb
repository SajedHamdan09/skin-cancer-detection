{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting TDA features...\n",
      "TDA shape: (583, 1300)\n",
      "Epoch 1/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 22ms/step - accuracy: 0.4827 - loss: 1.7430 - precision_1: 0.5430 - recall_1: 0.4220 - val_accuracy: 0.7542 - val_loss: 0.7422 - val_precision_1: 0.8216 - val_recall_1: 0.7054 - learning_rate: 1.0000e-04\n",
      "Epoch 2/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.7954 - loss: 0.6879 - precision_1: 0.8299 - recall_1: 0.7713 - val_accuracy: 0.8485 - val_loss: 0.5141 - val_precision_1: 0.8839 - val_recall_1: 0.7694 - learning_rate: 1.0000e-04\n",
      "Epoch 3/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.7944 - loss: 0.6170 - precision_1: 0.8192 - recall_1: 0.7720 - val_accuracy: 0.8923 - val_loss: 0.4081 - val_precision_1: 0.9167 - val_recall_1: 0.8519 - learning_rate: 1.0000e-04\n",
      "Epoch 4/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.8508 - loss: 0.4906 - precision_1: 0.8672 - recall_1: 0.8255 - val_accuracy: 0.8872 - val_loss: 0.3976 - val_precision_1: 0.9096 - val_recall_1: 0.8636 - learning_rate: 1.0000e-04\n",
      "Epoch 5/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.8560 - loss: 0.4494 - precision_1: 0.8712 - recall_1: 0.8383 - val_accuracy: 0.9108 - val_loss: 0.3242 - val_precision_1: 0.9304 - val_recall_1: 0.9007 - learning_rate: 1.0000e-04\n",
      "Epoch 6/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.8642 - loss: 0.4136 - precision_1: 0.8777 - recall_1: 0.8482 - val_accuracy: 0.9125 - val_loss: 0.2885 - val_precision_1: 0.9207 - val_recall_1: 0.8990 - learning_rate: 1.0000e-04\n",
      "Epoch 7/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.8580 - loss: 0.4233 - precision_1: 0.8769 - recall_1: 0.8468 - val_accuracy: 0.9074 - val_loss: 0.2869 - val_precision_1: 0.9134 - val_recall_1: 0.9057 - learning_rate: 1.0000e-04\n",
      "Epoch 8/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.8799 - loss: 0.3460 - precision_1: 0.8905 - recall_1: 0.8737 - val_accuracy: 0.9276 - val_loss: 0.2435 - val_precision_1: 0.9337 - val_recall_1: 0.9242 - learning_rate: 1.0000e-04\n",
      "Epoch 9/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.8733 - loss: 0.3603 - precision_1: 0.8843 - recall_1: 0.8611 - val_accuracy: 0.9175 - val_loss: 0.2643 - val_precision_1: 0.9296 - val_recall_1: 0.9108 - learning_rate: 1.0000e-04\n",
      "Epoch 10/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.8774 - loss: 0.3214 - precision_1: 0.9013 - recall_1: 0.8673 - val_accuracy: 0.9242 - val_loss: 0.2272 - val_precision_1: 0.9365 - val_recall_1: 0.9192 - learning_rate: 1.0000e-04\n",
      "Epoch 11/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.8951 - loss: 0.2975 - precision_1: 0.9082 - recall_1: 0.8850 - val_accuracy: 0.9209 - val_loss: 0.2587 - val_precision_1: 0.9314 - val_recall_1: 0.9141 - learning_rate: 1.0000e-04\n",
      "Epoch 12/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.8952 - loss: 0.3041 - precision_1: 0.9065 - recall_1: 0.8827 - val_accuracy: 0.9293 - val_loss: 0.2310 - val_precision_1: 0.9354 - val_recall_1: 0.9259 - learning_rate: 1.0000e-04\n",
      "Epoch 13/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.9197 - loss: 0.2393 - precision_1: 0.9230 - recall_1: 0.9099 - val_accuracy: 0.9209 - val_loss: 0.2181 - val_precision_1: 0.9282 - val_recall_1: 0.9141 - learning_rate: 1.0000e-04\n",
      "Epoch 14/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.9088 - loss: 0.2513 - precision_1: 0.9163 - recall_1: 0.9027 - val_accuracy: 0.9141 - val_loss: 0.2429 - val_precision_1: 0.9169 - val_recall_1: 0.9108 - learning_rate: 1.0000e-04\n",
      "Epoch 15/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.9115 - loss: 0.2596 - precision_1: 0.9196 - recall_1: 0.9048 - val_accuracy: 0.9276 - val_loss: 0.2283 - val_precision_1: 0.9317 - val_recall_1: 0.9192 - learning_rate: 1.0000e-04\n",
      "Epoch 16/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.9351 - loss: 0.1963 - precision_1: 0.9394 - recall_1: 0.9278 - val_accuracy: 0.9310 - val_loss: 0.1922 - val_precision_1: 0.9357 - val_recall_1: 0.9310 - learning_rate: 1.0000e-04\n",
      "Epoch 17/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.9036 - loss: 0.2474 - precision_1: 0.9121 - recall_1: 0.8974 - val_accuracy: 0.9343 - val_loss: 0.2023 - val_precision_1: 0.9358 - val_recall_1: 0.9327 - learning_rate: 1.0000e-04\n",
      "Epoch 18/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.9309 - loss: 0.1951 - precision_1: 0.9334 - recall_1: 0.9231 - val_accuracy: 0.9343 - val_loss: 0.1804 - val_precision_1: 0.9406 - val_recall_1: 0.9327 - learning_rate: 1.0000e-04\n",
      "Epoch 19/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.9189 - loss: 0.2178 - precision_1: 0.9244 - recall_1: 0.9160 - val_accuracy: 0.9293 - val_loss: 0.1967 - val_precision_1: 0.9309 - val_recall_1: 0.9293 - learning_rate: 1.0000e-04\n",
      "Epoch 20/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.9077 - loss: 0.2531 - precision_1: 0.9152 - recall_1: 0.9038 - val_accuracy: 0.9428 - val_loss: 0.1787 - val_precision_1: 0.9442 - val_recall_1: 0.9394 - learning_rate: 1.0000e-04\n",
      "Epoch 21/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.9223 - loss: 0.2033 - precision_1: 0.9297 - recall_1: 0.9142 - val_accuracy: 0.9411 - val_loss: 0.1756 - val_precision_1: 0.9473 - val_recall_1: 0.9377 - learning_rate: 1.0000e-04\n",
      "Epoch 22/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.9282 - loss: 0.2015 - precision_1: 0.9356 - recall_1: 0.9203 - val_accuracy: 0.9444 - val_loss: 0.1643 - val_precision_1: 0.9490 - val_recall_1: 0.9394 - learning_rate: 1.0000e-04\n",
      "Epoch 23/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.9258 - loss: 0.2031 - precision_1: 0.9313 - recall_1: 0.9218 - val_accuracy: 0.9444 - val_loss: 0.1671 - val_precision_1: 0.9490 - val_recall_1: 0.9394 - learning_rate: 1.0000e-04\n",
      "Epoch 24/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.9346 - loss: 0.1882 - precision_1: 0.9385 - recall_1: 0.9255 - val_accuracy: 0.9428 - val_loss: 0.1502 - val_precision_1: 0.9459 - val_recall_1: 0.9411 - learning_rate: 1.0000e-04\n",
      "Epoch 25/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.9355 - loss: 0.1612 - precision_1: 0.9414 - recall_1: 0.9316 - val_accuracy: 0.9495 - val_loss: 0.1370 - val_precision_1: 0.9510 - val_recall_1: 0.9478 - learning_rate: 1.0000e-04\n",
      "Epoch 26/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.9560 - loss: 0.1355 - precision_1: 0.9597 - recall_1: 0.9506 - val_accuracy: 0.9444 - val_loss: 0.1495 - val_precision_1: 0.9459 - val_recall_1: 0.9428 - learning_rate: 1.0000e-04\n",
      "Epoch 27/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.9391 - loss: 0.1630 - precision_1: 0.9416 - recall_1: 0.9348 - val_accuracy: 0.9478 - val_loss: 0.1404 - val_precision_1: 0.9494 - val_recall_1: 0.9478 - learning_rate: 1.0000e-04\n",
      "Epoch 28/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.9451 - loss: 0.1427 - precision_1: 0.9511 - recall_1: 0.9413 - val_accuracy: 0.9512 - val_loss: 0.1447 - val_precision_1: 0.9527 - val_recall_1: 0.9495 - learning_rate: 1.0000e-04\n",
      "Epoch 29/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.9524 - loss: 0.1375 - precision_1: 0.9558 - recall_1: 0.9491 - val_accuracy: 0.9478 - val_loss: 0.1579 - val_precision_1: 0.9476 - val_recall_1: 0.9444 - learning_rate: 1.0000e-04\n",
      "Epoch 30/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.9523 - loss: 0.1313 - precision_1: 0.9549 - recall_1: 0.9465 - val_accuracy: 0.9545 - val_loss: 0.1365 - val_precision_1: 0.9562 - val_recall_1: 0.9545 - learning_rate: 1.0000e-04\n",
      "Epoch 31/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.9433 - loss: 0.1464 - precision_1: 0.9479 - recall_1: 0.9359 - val_accuracy: 0.9478 - val_loss: 0.1522 - val_precision_1: 0.9510 - val_recall_1: 0.9478 - learning_rate: 1.0000e-04\n",
      "Epoch 32/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.9439 - loss: 0.1367 - precision_1: 0.9480 - recall_1: 0.9413 - val_accuracy: 0.9444 - val_loss: 0.1627 - val_precision_1: 0.9476 - val_recall_1: 0.9444 - learning_rate: 1.0000e-04\n",
      "Epoch 33/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.9404 - loss: 0.1564 - precision_1: 0.9447 - recall_1: 0.9358 - val_accuracy: 0.9512 - val_loss: 0.1252 - val_precision_1: 0.9526 - val_recall_1: 0.9478 - learning_rate: 1.0000e-04\n",
      "Epoch 34/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.9567 - loss: 0.1305 - precision_1: 0.9577 - recall_1: 0.9542 - val_accuracy: 0.9529 - val_loss: 0.1247 - val_precision_1: 0.9576 - val_recall_1: 0.9512 - learning_rate: 1.0000e-04\n",
      "Epoch 35/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.9498 - loss: 0.1319 - precision_1: 0.9539 - recall_1: 0.9452 - val_accuracy: 0.9545 - val_loss: 0.1207 - val_precision_1: 0.9593 - val_recall_1: 0.9512 - learning_rate: 1.0000e-04\n",
      "Epoch 36/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.9486 - loss: 0.1298 - precision_1: 0.9559 - recall_1: 0.9453 - val_accuracy: 0.9495 - val_loss: 0.1412 - val_precision_1: 0.9526 - val_recall_1: 0.9478 - learning_rate: 1.0000e-04\n",
      "Epoch 37/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.9470 - loss: 0.1289 - precision_1: 0.9510 - recall_1: 0.9452 - val_accuracy: 0.9478 - val_loss: 0.1636 - val_precision_1: 0.9508 - val_recall_1: 0.9444 - learning_rate: 1.0000e-04\n",
      "Epoch 38/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.9517 - loss: 0.1231 - precision_1: 0.9553 - recall_1: 0.9476 - val_accuracy: 0.9630 - val_loss: 0.1227 - val_precision_1: 0.9645 - val_recall_1: 0.9613 - learning_rate: 1.0000e-04\n",
      "Epoch 39/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.9532 - loss: 0.1346 - precision_1: 0.9552 - recall_1: 0.9517 - val_accuracy: 0.9613 - val_loss: 0.1303 - val_precision_1: 0.9661 - val_recall_1: 0.9596 - learning_rate: 1.0000e-04\n",
      "Epoch 40/100\n",
      "\u001b[1m35/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9504 - loss: 0.1237 - precision_1: 0.9520 - recall_1: 0.9438\n",
      "Epoch 40: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.9503 - loss: 0.1251 - precision_1: 0.9520 - recall_1: 0.9438 - val_accuracy: 0.9596 - val_loss: 0.1245 - val_precision_1: 0.9627 - val_recall_1: 0.9562 - learning_rate: 1.0000e-04\n",
      "Epoch 41/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.9652 - loss: 0.0927 - precision_1: 0.9684 - recall_1: 0.9618 - val_accuracy: 0.9579 - val_loss: 0.1234 - val_precision_1: 0.9611 - val_recall_1: 0.9579 - learning_rate: 5.0000e-05\n",
      "Epoch 42/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.9691 - loss: 0.0920 - precision_1: 0.9699 - recall_1: 0.9686 - val_accuracy: 0.9579 - val_loss: 0.1193 - val_precision_1: 0.9595 - val_recall_1: 0.9562 - learning_rate: 5.0000e-05\n",
      "Epoch 43/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.9577 - loss: 0.1062 - precision_1: 0.9616 - recall_1: 0.9540 - val_accuracy: 0.9562 - val_loss: 0.1188 - val_precision_1: 0.9578 - val_recall_1: 0.9562 - learning_rate: 5.0000e-05\n",
      "Epoch 44/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.9601 - loss: 0.1000 - precision_1: 0.9619 - recall_1: 0.9571 - val_accuracy: 0.9495 - val_loss: 0.1372 - val_precision_1: 0.9526 - val_recall_1: 0.9478 - learning_rate: 5.0000e-05\n",
      "Epoch 45/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.9664 - loss: 0.0936 - precision_1: 0.9674 - recall_1: 0.9617 - val_accuracy: 0.9579 - val_loss: 0.1305 - val_precision_1: 0.9595 - val_recall_1: 0.9562 - learning_rate: 5.0000e-05\n",
      "Epoch 46/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.9659 - loss: 0.0847 - precision_1: 0.9681 - recall_1: 0.9652 - val_accuracy: 0.9646 - val_loss: 0.1140 - val_precision_1: 0.9663 - val_recall_1: 0.9646 - learning_rate: 5.0000e-05\n",
      "Epoch 47/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.9689 - loss: 0.0854 - precision_1: 0.9697 - recall_1: 0.9655 - val_accuracy: 0.9630 - val_loss: 0.1047 - val_precision_1: 0.9629 - val_recall_1: 0.9613 - learning_rate: 5.0000e-05\n",
      "Epoch 48/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.9688 - loss: 0.0892 - precision_1: 0.9699 - recall_1: 0.9667 - val_accuracy: 0.9613 - val_loss: 0.1067 - val_precision_1: 0.9629 - val_recall_1: 0.9613 - learning_rate: 5.0000e-05\n",
      "Epoch 49/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.9664 - loss: 0.0949 - precision_1: 0.9677 - recall_1: 0.9643 - val_accuracy: 0.9596 - val_loss: 0.1110 - val_precision_1: 0.9596 - val_recall_1: 0.9596 - learning_rate: 5.0000e-05\n",
      "Epoch 50/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.9716 - loss: 0.0880 - precision_1: 0.9733 - recall_1: 0.9709 - val_accuracy: 0.9613 - val_loss: 0.1121 - val_precision_1: 0.9613 - val_recall_1: 0.9613 - learning_rate: 5.0000e-05\n",
      "Epoch 51/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.9609 - loss: 0.0917 - precision_1: 0.9636 - recall_1: 0.9603 - val_accuracy: 0.9613 - val_loss: 0.1100 - val_precision_1: 0.9629 - val_recall_1: 0.9613 - learning_rate: 5.0000e-05\n",
      "Epoch 52/100\n",
      "\u001b[1m37/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9639 - loss: 0.0897 - precision_1: 0.9659 - recall_1: 0.9609\n",
      "Epoch 52: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.9639 - loss: 0.0898 - precision_1: 0.9659 - recall_1: 0.9608 - val_accuracy: 0.9613 - val_loss: 0.1116 - val_precision_1: 0.9629 - val_recall_1: 0.9613 - learning_rate: 5.0000e-05\n",
      "Epoch 53/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.9615 - loss: 0.0947 - precision_1: 0.9619 - recall_1: 0.9605 - val_accuracy: 0.9579 - val_loss: 0.1164 - val_precision_1: 0.9611 - val_recall_1: 0.9579 - learning_rate: 2.5000e-05\n",
      "Epoch 54/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.9760 - loss: 0.0793 - precision_1: 0.9772 - recall_1: 0.9710 - val_accuracy: 0.9630 - val_loss: 0.1156 - val_precision_1: 0.9662 - val_recall_1: 0.9630 - learning_rate: 2.5000e-05\n",
      "Epoch 55/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.9725 - loss: 0.0700 - precision_1: 0.9733 - recall_1: 0.9706 - val_accuracy: 0.9697 - val_loss: 0.1084 - val_precision_1: 0.9695 - val_recall_1: 0.9646 - learning_rate: 2.5000e-05\n",
      "Epoch 56/100\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.9708 - loss: 0.0845 - precision_1: 0.9720 - recall_1: 0.9681 - val_accuracy: 0.9680 - val_loss: 0.1103 - val_precision_1: 0.9679 - val_recall_1: 0.9630 - learning_rate: 2.5000e-05\n",
      "Epoch 57/100\n",
      "\u001b[1m36/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9688 - loss: 0.0756 - precision_1: 0.9693 - recall_1: 0.9674\n",
      "Epoch 57: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.9692 - loss: 0.0753 - precision_1: 0.9697 - recall_1: 0.9677 - val_accuracy: 0.9613 - val_loss: 0.1100 - val_precision_1: 0.9661 - val_recall_1: 0.9596 - learning_rate: 2.5000e-05\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9621 - loss: 0.1050 - precision_1: 0.9620 - recall_1: 0.9581\n",
      "\n",
      "Test - Accuracy: 0.9630 | Precision: 0.9629 | Recall: 0.9613\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input, BatchNormalization, Add\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib  # for saving/loading scaler\n",
    "\n",
    "from gtda.homology import CubicalPersistence\n",
    "from gtda.diagrams import PersistenceImage, PersistenceLandscape\n",
    "\n",
    "# new dataset\n",
    "X_img = np.load(\"/home/sajedhamdan/Desktop/skin_cancer/CNN+TDA/new_dataset/images_train_256x192.npy\")\n",
    "y = np.load(\"/home/sajedhamdan/Desktop/skin_cancer/CNN+TDA/new_dataset/train_labels.npy\")\n",
    "X_img = X_img.astype(np.float32)\n",
    "\n",
    "sample_size = 728\n",
    "X_img, _, y, _ = train_test_split(X_img, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# TDA Feature Extraction\n",
    "def extract_tda_features(X_rgb):\n",
    "    X_gray = 0.2989 * X_rgb[..., 0] + 0.5870 * X_rgb[..., 1] + 0.1140 * X_rgb[..., 2]\n",
    "    cp = CubicalPersistence(homology_dimensions=[0, 1], n_jobs=-1)\n",
    "    diagrams = cp.fit_transform(X_gray)\n",
    "\n",
    "    pi = PersistenceImage(sigma=1.0, n_bins=20, weight_function=lambda x: x[1] ** 2)\n",
    "    pi_feat = pi.fit_transform(diagrams).reshape(len(diagrams), -1)\n",
    "\n",
    "    pl = PersistenceLandscape(n_layers=5, n_bins=50)\n",
    "    pl_feat = pl.fit_transform(diagrams).reshape(len(diagrams), -1)\n",
    "\n",
    "    return np.hstack((pi_feat, pl_feat))\n",
    "\n",
    "print(\"Extracting TDA features...\")\n",
    "X_tda = extract_tda_features(X_img)\n",
    "print(\"TDA shape:\", X_tda.shape)\n",
    "\n",
    "smote = SMOTE(k_neighbors=1, random_state=42)\n",
    "X_bal, y_bal = smote.fit_resample(X_tda, y)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_bal)\n",
    "\n",
    "# Save scaler for later use\n",
    "# joblib.dump(scaler, \"tda_feature_scaler.joblib\")\n",
    "\n",
    "y_cat = to_categorical(y_bal)\n",
    "\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_bal), y=y_bal)\n",
    "class_weight_dict = dict(enumerate(class_weights))\n",
    "\n",
    "# train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y_cat, test_size=0.2, stratify=y_cat, random_state=42\n",
    ")\n",
    "\n",
    "# MLP Model (ResNet-style)\n",
    "input_layer = Input(shape=(X_scaled.shape[1],))\n",
    "\n",
    "# block 1\n",
    "x = Dense(512, activation='relu')(input_layer)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "res1 = Dense(512)(x)\n",
    "res1 = BatchNormalization()(res1)\n",
    "\n",
    "# block 2\n",
    "x = Dense(512, activation='relu')(res1)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.3)(x)\n",
    "res2 = Add()([x, res1]) \n",
    "\n",
    "# block 3\n",
    "x = Dense(256, activation='relu')(res2)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.3)(x)\n",
    "res3 = Dense(256)(x)\n",
    "res3 = BatchNormalization()(res3)\n",
    "x = Add()([x, res3])  \n",
    "\n",
    "# output layer\n",
    "output_layer = Dense(6, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=1e-4),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]\n",
    ")\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "lr_schedule = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6, verbose=1)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=100,\n",
    "    batch_size=64,\n",
    "    validation_data=(X_test, y_test),\n",
    "    class_weight=class_weight_dict,\n",
    "    callbacks=[early_stop, lr_schedule],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "results = model.evaluate(X_test, y_test, verbose=1)\n",
    "print(f\"\\nTest - Accuracy: {results[1]:.4f} | Precision: {results[2]:.4f} | Recall: {results[3]:.4f}\")\n",
    "\n",
    "# save model for later use\n",
    "# model.save(\"tda_resnet_model_v2.keras\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
