{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-07 21:40:09.093667: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-06-07 21:40:09.256318: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1749321609.318034   57546 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1749321609.337438   57546 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1749321609.475410   57546 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1749321609.475440   57546 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1749321609.475442   57546 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1749321609.475443   57546 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-06-07 21:40:09.491538: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting TDA features...\n",
      "TDA shape: (2000, 1300)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-07 21:42:21.388075: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting CNN features...\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 2s/step\n",
      "CNN features shape: (2000, 2048)\n",
      "Balancing with SMOTE...\n",
      "Final input shape: (9366, 3348)\n",
      "Epoch 1/100\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 24ms/step - accuracy: 0.3313 - loss: 1.8066 - precision: 0.4285 - recall: 0.1374 - val_accuracy: 0.5912 - val_loss: 1.1527 - val_precision: 0.7821 - val_recall: 0.3122 - learning_rate: 1.0000e-04\n",
      "Epoch 2/100\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - accuracy: 0.5136 - loss: 1.3177 - precision: 0.6503 - recall: 0.2943 - val_accuracy: 0.6393 - val_loss: 1.0195 - val_precision: 0.7822 - val_recall: 0.4178 - learning_rate: 1.0000e-04\n",
      "Epoch 3/100\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 23ms/step - accuracy: 0.5685 - loss: 1.1596 - precision: 0.7073 - recall: 0.3709 - val_accuracy: 0.6708 - val_loss: 0.9142 - val_precision: 0.8146 - val_recall: 0.5016 - learning_rate: 1.0000e-04\n",
      "Epoch 4/100\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 23ms/step - accuracy: 0.6200 - loss: 1.0386 - precision: 0.7575 - recall: 0.4508 - val_accuracy: 0.7022 - val_loss: 0.8467 - val_precision: 0.8271 - val_recall: 0.5336 - learning_rate: 1.0000e-04\n",
      "Epoch 5/100\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - accuracy: 0.6393 - loss: 0.9881 - precision: 0.7626 - recall: 0.4804 - val_accuracy: 0.7236 - val_loss: 0.7634 - val_precision: 0.8392 - val_recall: 0.5902 - learning_rate: 1.0000e-04\n",
      "Epoch 6/100\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 23ms/step - accuracy: 0.6788 - loss: 0.8985 - precision: 0.7882 - recall: 0.5304 - val_accuracy: 0.7508 - val_loss: 0.7140 - val_precision: 0.8652 - val_recall: 0.6270 - learning_rate: 1.0000e-04\n",
      "Epoch 7/100\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - accuracy: 0.7006 - loss: 0.8572 - precision: 0.7961 - recall: 0.5626 - val_accuracy: 0.7657 - val_loss: 0.6670 - val_precision: 0.8668 - val_recall: 0.6457 - learning_rate: 1.0000e-04\n",
      "Epoch 8/100\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 21ms/step - accuracy: 0.7137 - loss: 0.8038 - precision: 0.8139 - recall: 0.5884 - val_accuracy: 0.7668 - val_loss: 0.6503 - val_precision: 0.8541 - val_recall: 0.6686 - learning_rate: 1.0000e-04\n",
      "Epoch 9/100\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - accuracy: 0.7258 - loss: 0.7614 - precision: 0.8122 - recall: 0.6135 - val_accuracy: 0.7940 - val_loss: 0.5967 - val_precision: 0.8666 - val_recall: 0.7001 - learning_rate: 1.0000e-04\n",
      "Epoch 10/100\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 23ms/step - accuracy: 0.7431 - loss: 0.7362 - precision: 0.8261 - recall: 0.6358 - val_accuracy: 0.8084 - val_loss: 0.5569 - val_precision: 0.8827 - val_recall: 0.7188 - learning_rate: 1.0000e-04\n",
      "Epoch 11/100\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - accuracy: 0.7466 - loss: 0.7343 - precision: 0.8218 - recall: 0.6556 - val_accuracy: 0.8122 - val_loss: 0.5403 - val_precision: 0.8762 - val_recall: 0.7289 - learning_rate: 1.0000e-04\n",
      "Epoch 12/100\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - accuracy: 0.7651 - loss: 0.6614 - precision: 0.8434 - recall: 0.6759 - val_accuracy: 0.8255 - val_loss: 0.5182 - val_precision: 0.8789 - val_recall: 0.7439 - learning_rate: 1.0000e-04\n",
      "Epoch 13/100\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - accuracy: 0.7754 - loss: 0.6478 - precision: 0.8426 - recall: 0.6910 - val_accuracy: 0.8287 - val_loss: 0.4995 - val_precision: 0.8889 - val_recall: 0.7513 - learning_rate: 1.0000e-04\n",
      "Epoch 14/100\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - accuracy: 0.7835 - loss: 0.6366 - precision: 0.8387 - recall: 0.6986 - val_accuracy: 0.8276 - val_loss: 0.4926 - val_precision: 0.8811 - val_recall: 0.7668 - learning_rate: 1.0000e-04\n",
      "Epoch 15/100\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - accuracy: 0.7827 - loss: 0.6106 - precision: 0.8439 - recall: 0.7010 - val_accuracy: 0.8501 - val_loss: 0.4528 - val_precision: 0.8947 - val_recall: 0.7801 - learning_rate: 1.0000e-04\n",
      "Epoch 16/100\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 23ms/step - accuracy: 0.8010 - loss: 0.5815 - precision: 0.8541 - recall: 0.7181 - val_accuracy: 0.8463 - val_loss: 0.4439 - val_precision: 0.8892 - val_recall: 0.7796 - learning_rate: 1.0000e-04\n",
      "Epoch 17/100\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 21ms/step - accuracy: 0.8076 - loss: 0.5591 - precision: 0.8651 - recall: 0.7372 - val_accuracy: 0.8415 - val_loss: 0.4465 - val_precision: 0.8832 - val_recall: 0.7871 - learning_rate: 1.0000e-04\n",
      "Epoch 18/100\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - accuracy: 0.8009 - loss: 0.5637 - precision: 0.8569 - recall: 0.7317 - val_accuracy: 0.8543 - val_loss: 0.4237 - val_precision: 0.8983 - val_recall: 0.8058 - learning_rate: 1.0000e-04\n",
      "Epoch 19/100\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - accuracy: 0.8234 - loss: 0.5096 - precision: 0.8627 - recall: 0.7517 - val_accuracy: 0.8629 - val_loss: 0.4008 - val_precision: 0.8986 - val_recall: 0.8180 - learning_rate: 1.0000e-04\n",
      "Epoch 20/100\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 21ms/step - accuracy: 0.8352 - loss: 0.4854 - precision: 0.8801 - recall: 0.7752 - val_accuracy: 0.8549 - val_loss: 0.3964 - val_precision: 0.8871 - val_recall: 0.8132 - learning_rate: 1.0000e-04\n",
      "Epoch 21/100\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 21ms/step - accuracy: 0.8268 - loss: 0.4937 - precision: 0.8661 - recall: 0.7748 - val_accuracy: 0.8629 - val_loss: 0.4040 - val_precision: 0.8985 - val_recall: 0.8127 - learning_rate: 1.0000e-04\n",
      "Epoch 22/100\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 23ms/step - accuracy: 0.8375 - loss: 0.4654 - precision: 0.8811 - recall: 0.7853 - val_accuracy: 0.8639 - val_loss: 0.3697 - val_precision: 0.8987 - val_recall: 0.8335 - learning_rate: 1.0000e-04\n",
      "Epoch 23/100\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - accuracy: 0.8396 - loss: 0.4488 - precision: 0.8818 - recall: 0.7889 - val_accuracy: 0.8693 - val_loss: 0.3716 - val_precision: 0.8963 - val_recall: 0.8394 - learning_rate: 1.0000e-04\n",
      "Epoch 24/100\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - accuracy: 0.8413 - loss: 0.4707 - precision: 0.8823 - recall: 0.7929 - val_accuracy: 0.8767 - val_loss: 0.3468 - val_precision: 0.9055 - val_recall: 0.8490 - learning_rate: 1.0000e-04\n",
      "Epoch 25/100\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 21ms/step - accuracy: 0.8522 - loss: 0.4365 - precision: 0.8868 - recall: 0.8090 - val_accuracy: 0.8794 - val_loss: 0.3378 - val_precision: 0.9071 - val_recall: 0.8442 - learning_rate: 1.0000e-04\n",
      "Epoch 26/100\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - accuracy: 0.8581 - loss: 0.4185 - precision: 0.8972 - recall: 0.8146 - val_accuracy: 0.8906 - val_loss: 0.3253 - val_precision: 0.9160 - val_recall: 0.8607 - learning_rate: 1.0000e-04\n",
      "Epoch 27/100\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - accuracy: 0.8516 - loss: 0.4307 - precision: 0.8854 - recall: 0.8059 - val_accuracy: 0.8858 - val_loss: 0.3128 - val_precision: 0.9148 - val_recall: 0.8597 - learning_rate: 1.0000e-04\n",
      "Epoch 28/100\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - accuracy: 0.8566 - loss: 0.4105 - precision: 0.8898 - recall: 0.8185 - val_accuracy: 0.8847 - val_loss: 0.3245 - val_precision: 0.9083 - val_recall: 0.8559 - learning_rate: 1.0000e-04\n",
      "Epoch 29/100\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - accuracy: 0.8785 - loss: 0.3675 - precision: 0.9082 - recall: 0.8471 - val_accuracy: 0.8901 - val_loss: 0.3086 - val_precision: 0.9104 - val_recall: 0.8618 - learning_rate: 1.0000e-04\n",
      "Epoch 30/100\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - accuracy: 0.8760 - loss: 0.3538 - precision: 0.9027 - recall: 0.8373 - val_accuracy: 0.8954 - val_loss: 0.3031 - val_precision: 0.9160 - val_recall: 0.8666 - learning_rate: 1.0000e-04\n",
      "Epoch 31/100\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - accuracy: 0.8645 - loss: 0.3827 - precision: 0.8955 - recall: 0.8291 - val_accuracy: 0.8885 - val_loss: 0.3043 - val_precision: 0.9136 - val_recall: 0.8581 - learning_rate: 1.0000e-04\n",
      "Epoch 32/100\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - accuracy: 0.8833 - loss: 0.3364 - precision: 0.9083 - recall: 0.8502 - val_accuracy: 0.8954 - val_loss: 0.2962 - val_precision: 0.9148 - val_recall: 0.8714 - learning_rate: 1.0000e-04\n",
      "Epoch 33/100\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - accuracy: 0.8899 - loss: 0.3247 - precision: 0.9156 - recall: 0.8589 - val_accuracy: 0.8922 - val_loss: 0.2922 - val_precision: 0.9135 - val_recall: 0.8682 - learning_rate: 1.0000e-04\n",
      "Epoch 34/100\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - accuracy: 0.8846 - loss: 0.3430 - precision: 0.9107 - recall: 0.8528 - val_accuracy: 0.9023 - val_loss: 0.2730 - val_precision: 0.9241 - val_recall: 0.8773 - learning_rate: 1.0000e-04\n",
      "Epoch 35/100\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 23ms/step - accuracy: 0.8850 - loss: 0.3322 - precision: 0.9090 - recall: 0.8548 - val_accuracy: 0.9045 - val_loss: 0.2731 - val_precision: 0.9220 - val_recall: 0.8826 - learning_rate: 1.0000e-04\n",
      "Epoch 36/100\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - accuracy: 0.8809 - loss: 0.3636 - precision: 0.9073 - recall: 0.8472 - val_accuracy: 0.9072 - val_loss: 0.2586 - val_precision: 0.9276 - val_recall: 0.8885 - learning_rate: 1.0000e-04\n",
      "Epoch 37/100\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 21ms/step - accuracy: 0.8964 - loss: 0.3173 - precision: 0.9183 - recall: 0.8653 - val_accuracy: 0.9050 - val_loss: 0.2624 - val_precision: 0.9223 - val_recall: 0.8869 - learning_rate: 1.0000e-04\n",
      "Epoch 38/100\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - accuracy: 0.9023 - loss: 0.2998 - precision: 0.9210 - recall: 0.8799 - val_accuracy: 0.9104 - val_loss: 0.2524 - val_precision: 0.9286 - val_recall: 0.8885 - learning_rate: 1.0000e-04\n",
      "Epoch 39/100\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - accuracy: 0.9055 - loss: 0.2923 - precision: 0.9231 - recall: 0.8808 - val_accuracy: 0.9130 - val_loss: 0.2552 - val_precision: 0.9324 - val_recall: 0.8906 - learning_rate: 1.0000e-04\n",
      "Epoch 40/100\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - accuracy: 0.9039 - loss: 0.2866 - precision: 0.9225 - recall: 0.8790 - val_accuracy: 0.9178 - val_loss: 0.2438 - val_precision: 0.9335 - val_recall: 0.8986 - learning_rate: 1.0000e-04\n",
      "Epoch 41/100\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 23ms/step - accuracy: 0.8978 - loss: 0.3121 - precision: 0.9188 - recall: 0.8759 - val_accuracy: 0.9162 - val_loss: 0.2375 - val_precision: 0.9309 - val_recall: 0.8986 - learning_rate: 1.0000e-04\n",
      "Epoch 42/100\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 23ms/step - accuracy: 0.9042 - loss: 0.2812 - precision: 0.9240 - recall: 0.8826 - val_accuracy: 0.9200 - val_loss: 0.2320 - val_precision: 0.9311 - val_recall: 0.9018 - learning_rate: 1.0000e-04\n",
      "Epoch 43/100\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - accuracy: 0.9002 - loss: 0.2893 - precision: 0.9214 - recall: 0.8764 - val_accuracy: 0.9157 - val_loss: 0.2310 - val_precision: 0.9288 - val_recall: 0.8975 - learning_rate: 1.0000e-04\n",
      "Epoch 44/100\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - accuracy: 0.9085 - loss: 0.2654 - precision: 0.9299 - recall: 0.8918 - val_accuracy: 0.9194 - val_loss: 0.2325 - val_precision: 0.9305 - val_recall: 0.9002 - learning_rate: 1.0000e-04\n",
      "Epoch 45/100\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - accuracy: 0.9173 - loss: 0.2523 - precision: 0.9320 - recall: 0.8945 - val_accuracy: 0.9205 - val_loss: 0.2243 - val_precision: 0.9347 - val_recall: 0.9018 - learning_rate: 1.0000e-04\n",
      "Epoch 46/100\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - accuracy: 0.9089 - loss: 0.2631 - precision: 0.9261 - recall: 0.8884 - val_accuracy: 0.9189 - val_loss: 0.2191 - val_precision: 0.9320 - val_recall: 0.9002 - learning_rate: 1.0000e-04\n",
      "Epoch 47/100\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 23ms/step - accuracy: 0.9155 - loss: 0.2397 - precision: 0.9349 - recall: 0.8995 - val_accuracy: 0.9162 - val_loss: 0.2275 - val_precision: 0.9309 - val_recall: 0.8991 - learning_rate: 1.0000e-04\n",
      "Epoch 48/100\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 23ms/step - accuracy: 0.8958 - loss: 0.2946 - precision: 0.9139 - recall: 0.8769 - val_accuracy: 0.9274 - val_loss: 0.2186 - val_precision: 0.9412 - val_recall: 0.9141 - learning_rate: 1.0000e-04\n",
      "Epoch 49/100\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - accuracy: 0.9199 - loss: 0.2474 - precision: 0.9368 - recall: 0.9049 - val_accuracy: 0.9237 - val_loss: 0.2179 - val_precision: 0.9326 - val_recall: 0.9088 - learning_rate: 1.0000e-04\n",
      "Epoch 50/100\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 23ms/step - accuracy: 0.9228 - loss: 0.2410 - precision: 0.9355 - recall: 0.9031 - val_accuracy: 0.9248 - val_loss: 0.2119 - val_precision: 0.9337 - val_recall: 0.9088 - learning_rate: 1.0000e-04\n",
      "Epoch 51/100\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - accuracy: 0.9273 - loss: 0.2222 - precision: 0.9405 - recall: 0.9115 - val_accuracy: 0.9328 - val_loss: 0.2031 - val_precision: 0.9429 - val_recall: 0.9157 - learning_rate: 1.0000e-04\n",
      "Epoch 52/100\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - accuracy: 0.9285 - loss: 0.2180 - precision: 0.9410 - recall: 0.9138 - val_accuracy: 0.9242 - val_loss: 0.2214 - val_precision: 0.9390 - val_recall: 0.9120 - learning_rate: 1.0000e-04\n",
      "Epoch 53/100\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - accuracy: 0.9221 - loss: 0.2312 - precision: 0.9363 - recall: 0.9114 - val_accuracy: 0.9157 - val_loss: 0.2452 - val_precision: 0.9301 - val_recall: 0.9018 - learning_rate: 1.0000e-04\n",
      "Epoch 54/100\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 23ms/step - accuracy: 0.9260 - loss: 0.2330 - precision: 0.9401 - recall: 0.9109 - val_accuracy: 0.9386 - val_loss: 0.1912 - val_precision: 0.9442 - val_recall: 0.9301 - learning_rate: 1.0000e-04\n",
      "Epoch 55/100\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 23ms/step - accuracy: 0.9241 - loss: 0.2256 - precision: 0.9360 - recall: 0.9092 - val_accuracy: 0.9312 - val_loss: 0.1965 - val_precision: 0.9430 - val_recall: 0.9189 - learning_rate: 1.0000e-04\n",
      "Epoch 56/100\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - accuracy: 0.9212 - loss: 0.2403 - precision: 0.9369 - recall: 0.9014 - val_accuracy: 0.9301 - val_loss: 0.2058 - val_precision: 0.9372 - val_recall: 0.9162 - learning_rate: 1.0000e-04\n",
      "Epoch 57/100\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - accuracy: 0.9225 - loss: 0.2183 - precision: 0.9373 - recall: 0.9078 - val_accuracy: 0.9418 - val_loss: 0.1842 - val_precision: 0.9493 - val_recall: 0.9296 - learning_rate: 1.0000e-04\n",
      "Epoch 58/100\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - accuracy: 0.9359 - loss: 0.1919 - precision: 0.9460 - recall: 0.9226 - val_accuracy: 0.9424 - val_loss: 0.1758 - val_precision: 0.9499 - val_recall: 0.9306 - learning_rate: 1.0000e-04\n",
      "Epoch 59/100\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - accuracy: 0.9316 - loss: 0.2116 - precision: 0.9458 - recall: 0.9182 - val_accuracy: 0.9402 - val_loss: 0.1827 - val_precision: 0.9486 - val_recall: 0.9258 - learning_rate: 1.0000e-04\n",
      "Epoch 60/100\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - accuracy: 0.9336 - loss: 0.2037 - precision: 0.9417 - recall: 0.9216 - val_accuracy: 0.9392 - val_loss: 0.1766 - val_precision: 0.9449 - val_recall: 0.9338 - learning_rate: 1.0000e-04\n",
      "Epoch 61/100\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 23ms/step - accuracy: 0.9253 - loss: 0.2253 - precision: 0.9344 - recall: 0.9170 - val_accuracy: 0.9424 - val_loss: 0.1839 - val_precision: 0.9499 - val_recall: 0.9306 - learning_rate: 1.0000e-04\n",
      "Epoch 62/100\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - accuracy: 0.9360 - loss: 0.2071 - precision: 0.9463 - recall: 0.9245 - val_accuracy: 0.9429 - val_loss: 0.1808 - val_precision: 0.9508 - val_recall: 0.9280 - learning_rate: 1.0000e-04\n",
      "Epoch 63/100\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - accuracy: 0.9375 - loss: 0.1864 - precision: 0.9455 - recall: 0.9280 - val_accuracy: 0.9418 - val_loss: 0.1752 - val_precision: 0.9489 - val_recall: 0.9322 - learning_rate: 1.0000e-04\n",
      "Epoch 64/100\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 21ms/step - accuracy: 0.9382 - loss: 0.1911 - precision: 0.9457 - recall: 0.9282 - val_accuracy: 0.9376 - val_loss: 0.1783 - val_precision: 0.9487 - val_recall: 0.9269 - learning_rate: 1.0000e-04\n",
      "Epoch 65/100\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - accuracy: 0.9422 - loss: 0.1721 - precision: 0.9542 - recall: 0.9313 - val_accuracy: 0.9440 - val_loss: 0.1736 - val_precision: 0.9490 - val_recall: 0.9338 - learning_rate: 1.0000e-04\n",
      "Epoch 66/100\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - accuracy: 0.9451 - loss: 0.1732 - precision: 0.9561 - recall: 0.9333 - val_accuracy: 0.9472 - val_loss: 0.1653 - val_precision: 0.9549 - val_recall: 0.9376 - learning_rate: 1.0000e-04\n",
      "Epoch 67/100\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 23ms/step - accuracy: 0.9451 - loss: 0.1701 - precision: 0.9543 - recall: 0.9356 - val_accuracy: 0.9504 - val_loss: 0.1565 - val_precision: 0.9542 - val_recall: 0.9440 - learning_rate: 1.0000e-04\n",
      "Epoch 68/100\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - accuracy: 0.9466 - loss: 0.1625 - precision: 0.9559 - recall: 0.9386 - val_accuracy: 0.9429 - val_loss: 0.1774 - val_precision: 0.9495 - val_recall: 0.9338 - learning_rate: 1.0000e-04\n",
      "Epoch 69/100\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - accuracy: 0.9450 - loss: 0.1619 - precision: 0.9519 - recall: 0.9355 - val_accuracy: 0.9440 - val_loss: 0.1625 - val_precision: 0.9496 - val_recall: 0.9360 - learning_rate: 1.0000e-04\n",
      "Epoch 70/100\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - accuracy: 0.9465 - loss: 0.1661 - precision: 0.9554 - recall: 0.9368 - val_accuracy: 0.9509 - val_loss: 0.1531 - val_precision: 0.9546 - val_recall: 0.9424 - learning_rate: 1.0000e-04\n",
      "Epoch 71/100\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - accuracy: 0.9515 - loss: 0.1560 - precision: 0.9585 - recall: 0.9430 - val_accuracy: 0.9461 - val_loss: 0.1607 - val_precision: 0.9545 - val_recall: 0.9397 - learning_rate: 1.0000e-04\n",
      "Epoch 72/100\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - accuracy: 0.9534 - loss: 0.1508 - precision: 0.9622 - recall: 0.9439 - val_accuracy: 0.9466 - val_loss: 0.1687 - val_precision: 0.9521 - val_recall: 0.9440 - learning_rate: 1.0000e-04\n",
      "Epoch 73/100\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 23ms/step - accuracy: 0.9486 - loss: 0.1618 - precision: 0.9543 - recall: 0.9394 - val_accuracy: 0.9525 - val_loss: 0.1458 - val_precision: 0.9578 - val_recall: 0.9450 - learning_rate: 1.0000e-04\n",
      "Epoch 74/100\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 23ms/step - accuracy: 0.9459 - loss: 0.1613 - precision: 0.9523 - recall: 0.9365 - val_accuracy: 0.9477 - val_loss: 0.1577 - val_precision: 0.9539 - val_recall: 0.9376 - learning_rate: 1.0000e-04\n",
      "Epoch 75/100\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 21ms/step - accuracy: 0.9472 - loss: 0.1687 - precision: 0.9547 - recall: 0.9396 - val_accuracy: 0.9392 - val_loss: 0.1781 - val_precision: 0.9439 - val_recall: 0.9344 - learning_rate: 1.0000e-04\n",
      "Epoch 76/100\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - accuracy: 0.9465 - loss: 0.1644 - precision: 0.9527 - recall: 0.9392 - val_accuracy: 0.9520 - val_loss: 0.1525 - val_precision: 0.9593 - val_recall: 0.9440 - learning_rate: 1.0000e-04\n",
      "Epoch 77/100\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - accuracy: 0.9303 - loss: 0.2078 - precision: 0.9401 - recall: 0.9235 - val_accuracy: 0.9509 - val_loss: 0.1680 - val_precision: 0.9536 - val_recall: 0.9434 - learning_rate: 1.0000e-04\n",
      "Epoch 78/100\n",
      "\u001b[1m117/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9496 - loss: 0.1562 - precision: 0.9571 - recall: 0.9421\n",
      "Epoch 78: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - accuracy: 0.9496 - loss: 0.1562 - precision: 0.9571 - recall: 0.9421 - val_accuracy: 0.9434 - val_loss: 0.1697 - val_precision: 0.9506 - val_recall: 0.9349 - learning_rate: 1.0000e-04\n",
      "Epoch 79/100\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - accuracy: 0.9543 - loss: 0.1460 - precision: 0.9613 - recall: 0.9467 - val_accuracy: 0.9578 - val_loss: 0.1421 - val_precision: 0.9599 - val_recall: 0.9461 - learning_rate: 5.0000e-05\n",
      "Epoch 80/100\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 23ms/step - accuracy: 0.9637 - loss: 0.1185 - precision: 0.9688 - recall: 0.9577 - val_accuracy: 0.9568 - val_loss: 0.1350 - val_precision: 0.9590 - val_recall: 0.9493 - learning_rate: 5.0000e-05\n",
      "Epoch 81/100\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - accuracy: 0.9613 - loss: 0.1228 - precision: 0.9672 - recall: 0.9546 - val_accuracy: 0.9594 - val_loss: 0.1366 - val_precision: 0.9649 - val_recall: 0.9541 - learning_rate: 5.0000e-05\n",
      "Epoch 82/100\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - accuracy: 0.9673 - loss: 0.1045 - precision: 0.9713 - recall: 0.9620 - val_accuracy: 0.9568 - val_loss: 0.1429 - val_precision: 0.9596 - val_recall: 0.9514 - learning_rate: 5.0000e-05\n",
      "Epoch 83/100\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - accuracy: 0.9612 - loss: 0.1141 - precision: 0.9646 - recall: 0.9558 - val_accuracy: 0.9530 - val_loss: 0.1444 - val_precision: 0.9575 - val_recall: 0.9504 - learning_rate: 5.0000e-05\n",
      "Epoch 84/100\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - accuracy: 0.9642 - loss: 0.1178 - precision: 0.9699 - recall: 0.9600 - val_accuracy: 0.9578 - val_loss: 0.1332 - val_precision: 0.9603 - val_recall: 0.9541 - learning_rate: 5.0000e-05\n",
      "Epoch 85/100\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - accuracy: 0.9639 - loss: 0.1086 - precision: 0.9700 - recall: 0.9579 - val_accuracy: 0.9589 - val_loss: 0.1396 - val_precision: 0.9634 - val_recall: 0.9541 - learning_rate: 5.0000e-05\n",
      "Epoch 86/100\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 23ms/step - accuracy: 0.9658 - loss: 0.1085 - precision: 0.9710 - recall: 0.9611 - val_accuracy: 0.9584 - val_loss: 0.1268 - val_precision: 0.9613 - val_recall: 0.9541 - learning_rate: 5.0000e-05\n",
      "Epoch 87/100\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 23ms/step - accuracy: 0.9687 - loss: 0.0983 - precision: 0.9737 - recall: 0.9644 - val_accuracy: 0.9584 - val_loss: 0.1289 - val_precision: 0.9618 - val_recall: 0.9552 - learning_rate: 5.0000e-05\n",
      "Epoch 88/100\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - accuracy: 0.9713 - loss: 0.0932 - precision: 0.9751 - recall: 0.9658 - val_accuracy: 0.9605 - val_loss: 0.1294 - val_precision: 0.9629 - val_recall: 0.9557 - learning_rate: 5.0000e-05\n",
      "Epoch 89/100\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 23ms/step - accuracy: 0.9691 - loss: 0.1040 - precision: 0.9738 - recall: 0.9647 - val_accuracy: 0.9589 - val_loss: 0.1356 - val_precision: 0.9619 - val_recall: 0.9557 - learning_rate: 5.0000e-05\n",
      "Epoch 90/100\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 23ms/step - accuracy: 0.9709 - loss: 0.0985 - precision: 0.9737 - recall: 0.9636 - val_accuracy: 0.9600 - val_loss: 0.1295 - val_precision: 0.9628 - val_recall: 0.9541 - learning_rate: 5.0000e-05\n",
      "Epoch 91/100\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9551 - loss: 0.1347 - precision: 0.9596 - recall: 0.9489\n",
      "Epoch 91: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - accuracy: 0.9551 - loss: 0.1347 - precision: 0.9597 - recall: 0.9489 - val_accuracy: 0.9546 - val_loss: 0.1381 - val_precision: 0.9580 - val_recall: 0.9482 - learning_rate: 5.0000e-05\n",
      "Epoch 92/100\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 23ms/step - accuracy: 0.9680 - loss: 0.1012 - precision: 0.9718 - recall: 0.9639 - val_accuracy: 0.9616 - val_loss: 0.1288 - val_precision: 0.9639 - val_recall: 0.9552 - learning_rate: 2.5000e-05\n",
      "Epoch 93/100\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 23ms/step - accuracy: 0.9712 - loss: 0.0917 - precision: 0.9752 - recall: 0.9652 - val_accuracy: 0.9610 - val_loss: 0.1250 - val_precision: 0.9639 - val_recall: 0.9557 - learning_rate: 2.5000e-05\n",
      "Epoch 94/100\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - accuracy: 0.9711 - loss: 0.0918 - precision: 0.9747 - recall: 0.9656 - val_accuracy: 0.9648 - val_loss: 0.1217 - val_precision: 0.9646 - val_recall: 0.9589 - learning_rate: 2.5000e-05\n",
      "Epoch 95/100\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 23ms/step - accuracy: 0.9719 - loss: 0.0869 - precision: 0.9746 - recall: 0.9688 - val_accuracy: 0.9648 - val_loss: 0.1208 - val_precision: 0.9662 - val_recall: 0.9605 - learning_rate: 2.5000e-05\n",
      "Epoch 96/100\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 23ms/step - accuracy: 0.9748 - loss: 0.0816 - precision: 0.9785 - recall: 0.9726 - val_accuracy: 0.9642 - val_loss: 0.1184 - val_precision: 0.9651 - val_recall: 0.9605 - learning_rate: 2.5000e-05\n",
      "Epoch 97/100\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 23ms/step - accuracy: 0.9755 - loss: 0.0851 - precision: 0.9784 - recall: 0.9712 - val_accuracy: 0.9632 - val_loss: 0.1238 - val_precision: 0.9651 - val_recall: 0.9594 - learning_rate: 2.5000e-05\n",
      "Epoch 98/100\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 23ms/step - accuracy: 0.9783 - loss: 0.0758 - precision: 0.9798 - recall: 0.9734 - val_accuracy: 0.9648 - val_loss: 0.1222 - val_precision: 0.9657 - val_recall: 0.9605 - learning_rate: 2.5000e-05\n",
      "Epoch 99/100\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 23ms/step - accuracy: 0.9762 - loss: 0.0790 - precision: 0.9792 - recall: 0.9728 - val_accuracy: 0.9642 - val_loss: 0.1197 - val_precision: 0.9657 - val_recall: 0.9605 - learning_rate: 2.5000e-05\n",
      "Epoch 100/100\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - accuracy: 0.9744 - loss: 0.0812 - precision: 0.9770 - recall: 0.9675 - val_accuracy: 0.9600 - val_loss: 0.1257 - val_precision: 0.9618 - val_recall: 0.9552 - learning_rate: 2.5000e-05\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9620 - loss: 0.1220 - precision: 0.9640 - recall: 0.9576\n",
      "Test - Accuracy: 0.9642 | Precision: 0.9651 | Recall: 0.9605\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input, GlobalAveragePooling2D\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from gtda.homology import CubicalPersistence\n",
    "from gtda.diagrams import PersistenceImage, PersistenceLandscape\n",
    "\n",
    "X_img = np.load(\"/home/sajedhamdan/Desktop/skin_cancer/images_train_256x192.npy\") \n",
    "y = np.load(\"/home/sajedhamdan/Desktop/skin_cancer/train_labels.npy\")            \n",
    "X_img = preprocess_input(X_img.astype(np.float32)) \n",
    "y_cat = to_categorical(y)\n",
    "\n",
    "sample_size = 2000\n",
    "X_img, _, y, _ = train_test_split(\n",
    "    X_img, y, train_size=sample_size, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "def extract_tda_features(X_rgb):\n",
    "    X_Epoch 41/100\n",
    "agrams = cp.fit_transform(X_gray)\n",
    "\n",
    "    pi = PersistenceImage(sigma=1.0, n_bins=20, weight_function=lambda x: x[1] ** 2)\n",
    "    pi_feat = pi.fit_transform(diagrams).reshape(len(diagrams), -1)\n",
    "\n",
    "    pl = PersistenceLandscape(n_layers=5, n_bins=50)\n",
    "    pl_feat = pl.fit_transform(diagrams).reshape(len(diagrams), -1)\n",
    "\n",
    "    return np.hstack((pi_feat, pl_feat))\n",
    "\n",
    "print(\"Extracting TDA features...\")\n",
    "X_tda_features = extract_tda_features(X_img)\n",
    "print(\"TDA shape:\", X_tda_features.shape)\n",
    "\n",
    "resnet_base = ResNet50(include_top=False, weights='imagenet', input_shape=(192, 256, 3))\n",
    "\n",
    "for layer in resnet_base.layers[:-50]: \n",
    "    layer.trainable = False\n",
    "\n",
    "for layer in resnet_base.layers[-50:]: \n",
    "    layer.trainable = True\n",
    "\n",
    "cnn_output = GlobalAveragePooling2D()(resnet_base.output)\n",
    "cnn_model = Model(resnet_base.input, cnn_output)\n",
    "\n",
    "print(\"Extracting CNN features...\")\n",
    "X_img_features = cnn_model.predict(X_img, batch_size=32, verbose=1)  \n",
    "print(\"CNN features shape:\", X_img_features.shape)\n",
    "\n",
    "X_combined = np.hstack((X_img_features, X_tda_features))\n",
    "\n",
    "print(\"Balancing with SMOTE...\")\n",
    "smote = SMOTE(random_state=42)\n",
    "\n",
    "y_subset = y[:X_img_features.shape[0]]\n",
    "\n",
    "X_balanced, y_bal = smote.fit_resample(X_combined, y_subset)\n",
    "\n",
    "\n",
    "X_img_bal = X_balanced[:, :X_img_features.shape[1]]\n",
    "X_tda_bal = X_balanced[:, X_img_features.shape[1]:]\n",
    "y_bal_cat = to_categorical(y_bal)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_tda_norm = scaler.fit_transform(X_tda_bal)\n",
    "\n",
    "X_final = np.concatenate([X_img_bal, X_tda_norm], axis=1)\n",
    "print(\"Final input shape:\", X_final.shape)\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_final, y_bal_cat, test_size=0.2, random_state=42, stratify=y_bal)\n",
    "\n",
    "# MLP classifier\n",
    "input_layer = Input(shape=(X_final.shape[1],))\n",
    "x = Dense(512, activation='relu')(input_layer)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = Dropout(0.3)(x)\n",
    "output_layer = Dense(7, activation='softmax')(x)\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5,\n",
    "                                 patience=5, min_lr=1e-6, verbose=1)\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=1e-4),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=100,\n",
    "    batch_size=64,\n",
    "    validation_data=(X_test, y_test),\n",
    "    callbacks=[early_stop, lr_scheduler],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "\n",
    "results = model.evaluate(X_test, y_test, verbose=1)\n",
    "print(f\"Test - Accuracy: {results[1]:.4f} | Precision: {results[2]:.4f} | Recall: {results[3]:.4f}\")\n",
    "\n",
    "model.save(\"tda_resnet_model_v1.keras\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
