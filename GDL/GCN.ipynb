{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.segmentation import slic\n",
    "from skimage.color import rgb2lab\n",
    "from skimage.graph import rag_mean_color \n",
    "from skimage.measure import regionprops\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from skimage import graph\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader as GeometricDataLoader\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "X_train_np = np.load(\"/home/sajedhamdan/Desktop/skin_cancer/images_train_256x192.npy\")\n",
    "y_train_np = np.load(\"/home/sajedhamdan/Desktop/skin_cancer/train_labels.npy\")\n",
    "\n",
    "X_val_np = np.load(\"/home/sajedhamdan/Desktop/skin_cancer/images_val_256x192.npy\")\n",
    "y_val_np = np.load(\"/home/sajedhamdan/Desktop/skin_cancer/val_labels.npy\")\n",
    "\n",
    "X_test_np = np.load(\"/home/sajedhamdan/Desktop/skin_cancer/images_test_256x192.npy\")\n",
    "y_test_np = np.load(\"/home/sajedhamdan/Desktop/skin_cancer/test_labels.npy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function to create graph data from an image\n",
    "def create_graph_from_image(image_tensor, label_tensor):\n",
    "    \n",
    "    image_np = image_tensor.permute(1, 2, 0).numpy() \n",
    "    label = label_tensor.item() \n",
    "\n",
    "    segments = slic(image_np, n_segments=100, compactness=10, sigma=1, channel_axis=-1, enforce_connectivity=True)\n",
    "\n",
    "    num_segments = np.max(segments) + 1\n",
    "    image_lab = rgb2lab(image_np)\n",
    "\n",
    "    node_features = np.zeros((num_segments, 6))\n",
    "\n",
    "    for i in range(num_segments):\n",
    "        mask = (segments == i)\n",
    "        lab_pixels_in_segment = image_lab[mask]\n",
    "    \n",
    "        if lab_pixels_in_segment.size > 0:\n",
    "            node_features[i, :3] = lab_pixels_in_segment.mean(axis=0) \n",
    "            node_features[i, 3:] = lab_pixels_in_segment.std(axis=0) \n",
    "        else:\n",
    "            node_features[i, :] = 0.0 \n",
    "\n",
    "    rag = graph.rag_mean_color(image_np, segments)\n",
    "    \n",
    "    edge_index = []\n",
    "    for node1, node2 in rag.edges():\n",
    "        edge_index.append([node1, node2])\n",
    "        edge_index.append([node2, node1])\n",
    "\n",
    "    if not edge_index:\n",
    "        edge_index = torch.empty((2, 0), dtype=torch.long)\n",
    "    else:\n",
    "        edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "\n",
    "    data = Data(x=torch.tensor(node_features, dtype=torch.float32), \n",
    "                edge_index=edge_index, \n",
    "                y=torch.tensor([label], dtype=torch.long))\n",
    "    return data\n",
    "\n",
    "\n",
    "class CustomGraphDataset(Dataset):\n",
    "    def __init__(self, image_data, label_data, scaler=None, train=True):\n",
    "        self.image_data = image_data  \n",
    "        self.label_data = label_data\n",
    "        self.graphs = []\n",
    "        self.scaler = scaler\n",
    "        self.train = train\n",
    "\n",
    "        print(f\"Generating graphs for {'training' if train else 'validation'} dataset:\")\n",
    "        for i in range(len(self.image_data)):\n",
    "           \n",
    "            img_tensor = torch.tensor(self.image_data[i], dtype=torch.float32).permute(2, 0, 1)\n",
    "            lbl_tensor = torch.tensor(self.label_data[i], dtype=torch.long)\n",
    "            graph_data = create_graph_from_image(img_tensor, lbl_tensor)\n",
    "\n",
    "            num_nodes = graph_data.x.size(0) \n",
    "\n",
    "            train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "            val_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "            test_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "\n",
    "            train_mask[:int(0.7 * num_nodes)] = 1 \n",
    "            val_mask[int(0.7 * num_nodes):int(0.85 * num_nodes)] = 1\n",
    "            test_mask[int(0.85 * num_nodes):] = 1 \n",
    "\n",
    "            graph_data.train_mask = train_mask\n",
    "            graph_data.val_mask = val_mask\n",
    "            graph_data.test_mask = test_mask\n",
    "\n",
    "            self.graphs.append(graph_data)\n",
    "\n",
    "        print(f\"Finished generating graphs for {'training' if train else 'validation'} dataset.\")\n",
    "\n",
    "        if self.train:  \n",
    "            self.fit_scaler()\n",
    "        \n",
    "        self.transform_data()\n",
    "\n",
    "    def fit_scaler(self):\n",
    "        \n",
    "        print(\"Fitting StandardScaler on training node features...\")\n",
    "        all_node_features = []\n",
    "        for graph_data in self.graphs:\n",
    "            if graph_data.x is not None and graph_data.x.numel() > 0:  \n",
    "                all_node_features.append(graph_data.x.numpy()) \n",
    "        \n",
    "        if all_node_features:\n",
    "            combined_features = np.vstack(all_node_features)\n",
    "            self.scaler = StandardScaler()\n",
    "            self.scaler.fit(combined_features)\n",
    "            print(\"StandardScaler fitted successfully.\")\n",
    "        else:\n",
    "            print(\"No node features found to fit scaler.\")\n",
    "\n",
    "    def transform_data(self):\n",
    "       \n",
    "        if self.scaler is not None:\n",
    "            print(f\"Transforming node features for {'training' if self.train else 'validation'} dataset:\")\n",
    "            for i, graph_data in enumerate(self.graphs):\n",
    "                if graph_data.x is not None and graph_data.x.numel() > 0:  \n",
    "                    transformed_features = self.scaler.transform(graph_data.x.numpy())\n",
    "                    self.graphs[i].x = torch.tensor(transformed_features, dtype=torch.float32)\n",
    "                else:\n",
    "                    print(f\"Warning: Graph {i} has no nodes (empty graph.x).\")\n",
    "            print(f\"Finished transforming node features for {'training' if self.train else 'validation'} dataset.\")\n",
    "        elif not self.train:\n",
    "            print(\"Warning: No scaler provided for validation dataset.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.graphs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.graphs[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating graphs for training dataset...\n",
      "Finished generating graphs for training dataset.\n",
      "Fitting StandardScaler on training node features...\n",
      "StandardScaler fitted successfully.\n",
      "Transforming node features for training dataset...\n",
      "Finished transforming node features for training dataset.\n",
      "Generating graphs for validation dataset...\n",
      "Finished generating graphs for validation dataset.\n",
      "Transforming node features for validation dataset...\n",
      "Finished transforming node features for validation dataset.\n",
      "\n",
      "--- Data Loading Complete ---\n",
      "Number of training graphs: 8111\n",
      "Number of validation graphs: 902\n",
      "Sample training batch (from DataLoader): DataBatch(x=[3415, 6], edge_index=[2, 17868], y=[32], train_mask=[3415], val_mask=[3415], test_mask=[3415], batch=[3415], ptr=[33])\n",
      "Shape of node features in sample batch: torch.Size([3415, 6])\n",
      "Number of graphs in sample batch: 32\n"
     ]
    }
   ],
   "source": [
    "# custom datasets\n",
    "train_dataset = CustomGraphDataset(X_train_np, y_train_np, train=True)\n",
    "val_dataset = CustomGraphDataset(X_val_np, y_val_np, scaler=train_dataset.scaler, train=False)\n",
    "\n",
    "# dataLoaders for graph data \n",
    "train_loader = GeometricDataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = GeometricDataLoader(val_dataset, batch_size=32)\n",
    "\n",
    "print(f\"Number of training graphs: {len(train_dataset)}\")\n",
    "print(f\"Number of validation graphs: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating graphs for validation dataset...\n",
      "Finished generating graphs for validation dataset.\n",
      "Transforming node features for validation dataset...\n",
      "Finished transforming node features for validation dataset.\n"
     ]
    }
   ],
   "source": [
    "test_dataset = CustomGraphDataset(X_test_np, y_test_np, scaler=train_dataset.scaler, train=False)\n",
    "test_loader = GeometricDataLoader(test_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.linear = torch.nn.Linear(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr=None, batch=None):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        if batch is not None:\n",
    "            x = global_mean_pool(x, batch)\n",
    "\n",
    "        out = self.linear(x)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = train_dataset.graphs[0].x.shape[1]  \n",
    "num_classes = len(set(train_dataset.label_data))   \n",
    "\n",
    "\n",
    "model = GCN(\n",
    "    in_channels=num_features,  \n",
    "    hidden_channels=args.hidden_channels,\n",
    "    out_channels=num_classes,  \n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam([\n",
    "    dict(params=model.conv1.parameters(), weight_decay=5e-4),  \n",
    "    dict(params=model.conv2.parameters(), weight_decay=0)],   \n",
    "    lr=0.0001)\n",
    "\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        out = model(data.x, data.edge_index, data.edge_attr, data.batch)\n",
    "        \n",
    "        loss = F.cross_entropy(out, data.y)\n",
    "        loss.backward()  \n",
    "        optimizer.step() \n",
    "        \n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "@torch.no_grad()\n",
    "def test():\n",
    "    model.eval() \n",
    "    \n",
    "    all_preds = [[], [], []] \n",
    "    all_labels = [[], [], []]  \n",
    "\n",
    "    for loader_idx, loader in enumerate([train_loader, val_loader, test_loader]):\n",
    "        for data in loader:\n",
    "            data = data.to(device)\n",
    "            out = model(data.x, data.edge_index, data.edge_attr, data.batch)\n",
    "            pred = out.argmax(dim=1)  \n",
    "            labels = data.y  \n",
    "\n",
    "            all_preds[loader_idx].extend(pred.cpu().tolist())\n",
    "            all_labels[loader_idx].extend(labels.cpu().tolist())\n",
    "\n",
    "    # accuracy, precision and recall for all datasets\n",
    "    results = []\n",
    "    for preds, labels in zip(all_preds, all_labels):\n",
    "        acc = (sum(p == l for p, l in zip(preds, labels)) / len(labels))\n",
    "        prec = precision_score(labels, preds, average='macro', zero_division=0) \n",
    "        rec = recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "Train Loss: 0.8175, Train Accuracy: 0.7029, Train Precision: 0.6493, Train Recall: 0.7029\n",
      "Validation Loss: 0.8403, Validation Accuracy: 0.6785, Validation Precision: 0.6056, Validation Recall: 0.6785\n",
      "Test Accuracy: 0.7046, Test Precision: 0.6716, Test Recall: 0.7046\n",
      "Epoch 2/50\n",
      "Train Loss: 0.8159, Train Accuracy: 0.7021, Train Precision: 0.6492, Train Recall: 0.7021\n",
      "Validation Loss: 0.8394, Validation Accuracy: 0.6785, Validation Precision: 0.6057, Validation Recall: 0.6785\n",
      "Test Accuracy: 0.7006, Test Precision: 0.6354, Test Recall: 0.7006\n",
      "Epoch 3/50\n",
      "Train Loss: 0.8161, Train Accuracy: 0.7030, Train Precision: 0.6487, Train Recall: 0.7030\n",
      "Validation Loss: 0.8388, Validation Accuracy: 0.6796, Validation Precision: 0.6081, Validation Recall: 0.6796\n",
      "Test Accuracy: 0.7056, Test Precision: 0.6452, Test Recall: 0.7056\n",
      "Epoch 4/50\n",
      "Train Loss: 0.8147, Train Accuracy: 0.7035, Train Precision: 0.6505, Train Recall: 0.7035\n",
      "Validation Loss: 0.8381, Validation Accuracy: 0.6796, Validation Precision: 0.6084, Validation Recall: 0.6796\n",
      "Test Accuracy: 0.7056, Test Precision: 0.6732, Test Recall: 0.7056\n",
      "Epoch 5/50\n",
      "Train Loss: 0.8136, Train Accuracy: 0.7039, Train Precision: 0.6514, Train Recall: 0.7039\n",
      "Validation Loss: 0.8373, Validation Accuracy: 0.6785, Validation Precision: 0.6074, Validation Recall: 0.6785\n",
      "Test Accuracy: 0.7016, Test Precision: 0.6659, Test Recall: 0.7016\n",
      "Epoch 6/50\n",
      "Train Loss: 0.8129, Train Accuracy: 0.7039, Train Precision: 0.6518, Train Recall: 0.7039\n",
      "Validation Loss: 0.8365, Validation Accuracy: 0.6785, Validation Precision: 0.6074, Validation Recall: 0.6785\n",
      "Test Accuracy: 0.7026, Test Precision: 0.6402, Test Recall: 0.7026\n",
      "Epoch 7/50\n",
      "Train Loss: 0.8126, Train Accuracy: 0.7041, Train Precision: 0.6517, Train Recall: 0.7041\n",
      "Validation Loss: 0.8359, Validation Accuracy: 0.6774, Validation Precision: 0.6066, Validation Recall: 0.6774\n",
      "Test Accuracy: 0.7026, Test Precision: 0.6396, Test Recall: 0.7026\n",
      "Epoch 8/50\n",
      "Train Loss: 0.8123, Train Accuracy: 0.7040, Train Precision: 0.6526, Train Recall: 0.7040\n",
      "Validation Loss: 0.8353, Validation Accuracy: 0.6774, Validation Precision: 0.6066, Validation Recall: 0.6774\n",
      "Test Accuracy: 0.7026, Test Precision: 0.6396, Test Recall: 0.7026\n",
      "Epoch 9/50\n",
      "Train Loss: 0.8118, Train Accuracy: 0.7035, Train Precision: 0.6506, Train Recall: 0.7035\n",
      "Validation Loss: 0.8345, Validation Accuracy: 0.6763, Validation Precision: 0.6069, Validation Recall: 0.6763\n",
      "Test Accuracy: 0.7046, Test Precision: 0.6432, Test Recall: 0.7046\n",
      "Epoch 10/50\n",
      "Train Loss: 0.8108, Train Accuracy: 0.7045, Train Precision: 0.6525, Train Recall: 0.7045\n",
      "Validation Loss: 0.8338, Validation Accuracy: 0.6785, Validation Precision: 0.6085, Validation Recall: 0.6785\n",
      "Test Accuracy: 0.7036, Test Precision: 0.6411, Test Recall: 0.7036\n",
      "Epoch 11/50\n",
      "Train Loss: 0.8103, Train Accuracy: 0.7040, Train Precision: 0.6515, Train Recall: 0.7040\n",
      "Validation Loss: 0.8331, Validation Accuracy: 0.6763, Validation Precision: 0.6068, Validation Recall: 0.6763\n",
      "Test Accuracy: 0.7046, Test Precision: 0.6428, Test Recall: 0.7046\n",
      "Epoch 12/50\n",
      "Train Loss: 0.8095, Train Accuracy: 0.7044, Train Precision: 0.6540, Train Recall: 0.7044\n",
      "Validation Loss: 0.8324, Validation Accuracy: 0.6763, Validation Precision: 0.6076, Validation Recall: 0.6763\n",
      "Test Accuracy: 0.7026, Test Precision: 0.6410, Test Recall: 0.7026\n",
      "Epoch 13/50\n",
      "Train Loss: 0.8093, Train Accuracy: 0.7047, Train Precision: 0.6554, Train Recall: 0.7047\n",
      "Validation Loss: 0.8317, Validation Accuracy: 0.6785, Validation Precision: 0.6095, Validation Recall: 0.6785\n",
      "Test Accuracy: 0.7046, Test Precision: 0.6428, Test Recall: 0.7046\n",
      "Epoch 14/50\n",
      "Train Loss: 0.8086, Train Accuracy: 0.7045, Train Precision: 0.6551, Train Recall: 0.7045\n",
      "Validation Loss: 0.8310, Validation Accuracy: 0.6785, Validation Precision: 0.6095, Validation Recall: 0.6785\n",
      "Test Accuracy: 0.7036, Test Precision: 0.6418, Test Recall: 0.7036\n",
      "Epoch 15/50\n",
      "Train Loss: 0.8074, Train Accuracy: 0.7051, Train Precision: 0.6549, Train Recall: 0.7051\n",
      "Validation Loss: 0.8305, Validation Accuracy: 0.6752, Validation Precision: 0.6078, Validation Recall: 0.6752\n",
      "Test Accuracy: 0.7016, Test Precision: 0.6406, Test Recall: 0.7016\n",
      "Epoch 16/50\n",
      "Train Loss: 0.8077, Train Accuracy: 0.7050, Train Precision: 0.6557, Train Recall: 0.7050\n",
      "Validation Loss: 0.8299, Validation Accuracy: 0.6763, Validation Precision: 0.6095, Validation Recall: 0.6763\n",
      "Test Accuracy: 0.7016, Test Precision: 0.6406, Test Recall: 0.7016\n",
      "Epoch 17/50\n",
      "Train Loss: 0.8053, Train Accuracy: 0.7051, Train Precision: 0.6552, Train Recall: 0.7051\n",
      "Validation Loss: 0.8293, Validation Accuracy: 0.6763, Validation Precision: 0.6095, Validation Recall: 0.6763\n",
      "Test Accuracy: 0.7016, Test Precision: 0.6406, Test Recall: 0.7016\n",
      "Epoch 18/50\n",
      "Train Loss: 0.8058, Train Accuracy: 0.7047, Train Precision: 0.6558, Train Recall: 0.7047\n",
      "Validation Loss: 0.8287, Validation Accuracy: 0.6785, Validation Precision: 0.6116, Validation Recall: 0.6785\n",
      "Test Accuracy: 0.7006, Test Precision: 0.6390, Test Recall: 0.7006\n",
      "Epoch 19/50\n",
      "Train Loss: 0.8048, Train Accuracy: 0.7046, Train Precision: 0.6556, Train Recall: 0.7046\n",
      "Validation Loss: 0.8282, Validation Accuracy: 0.6785, Validation Precision: 0.6102, Validation Recall: 0.6785\n",
      "Test Accuracy: 0.7006, Test Precision: 0.6392, Test Recall: 0.7006\n",
      "Epoch 20/50\n",
      "Train Loss: 0.8046, Train Accuracy: 0.7052, Train Precision: 0.6566, Train Recall: 0.7052\n",
      "Validation Loss: 0.8275, Validation Accuracy: 0.6785, Validation Precision: 0.6094, Validation Recall: 0.6785\n",
      "Test Accuracy: 0.7006, Test Precision: 0.6392, Test Recall: 0.7006\n",
      "Epoch 21/50\n",
      "Train Loss: 0.8044, Train Accuracy: 0.7048, Train Precision: 0.6557, Train Recall: 0.7048\n",
      "Validation Loss: 0.8269, Validation Accuracy: 0.6774, Validation Precision: 0.6100, Validation Recall: 0.6774\n",
      "Test Accuracy: 0.7006, Test Precision: 0.6394, Test Recall: 0.7006\n",
      "Epoch 22/50\n",
      "Train Loss: 0.8029, Train Accuracy: 0.7052, Train Precision: 0.6563, Train Recall: 0.7052\n",
      "Validation Loss: 0.8263, Validation Accuracy: 0.6774, Validation Precision: 0.6099, Validation Recall: 0.6774\n",
      "Test Accuracy: 0.7016, Test Precision: 0.6414, Test Recall: 0.7016\n",
      "Epoch 23/50\n",
      "Train Loss: 0.8025, Train Accuracy: 0.7052, Train Precision: 0.6564, Train Recall: 0.7052\n",
      "Validation Loss: 0.8256, Validation Accuracy: 0.6763, Validation Precision: 0.6080, Validation Recall: 0.6763\n",
      "Test Accuracy: 0.7016, Test Precision: 0.6412, Test Recall: 0.7016\n",
      "Epoch 24/50\n",
      "Train Loss: 0.8013, Train Accuracy: 0.7050, Train Precision: 0.6575, Train Recall: 0.7050\n",
      "Validation Loss: 0.8252, Validation Accuracy: 0.6774, Validation Precision: 0.6081, Validation Recall: 0.6774\n",
      "Test Accuracy: 0.7006, Test Precision: 0.6392, Test Recall: 0.7006\n",
      "Epoch 25/50\n",
      "Train Loss: 0.8018, Train Accuracy: 0.7052, Train Precision: 0.6568, Train Recall: 0.7052\n",
      "Validation Loss: 0.8246, Validation Accuracy: 0.6752, Validation Precision: 0.6072, Validation Recall: 0.6752\n",
      "Test Accuracy: 0.7006, Test Precision: 0.6393, Test Recall: 0.7006\n",
      "Epoch 26/50\n",
      "Train Loss: 0.8016, Train Accuracy: 0.7060, Train Precision: 0.6578, Train Recall: 0.7060\n",
      "Validation Loss: 0.8241, Validation Accuracy: 0.6763, Validation Precision: 0.6094, Validation Recall: 0.6763\n",
      "Test Accuracy: 0.7016, Test Precision: 0.6412, Test Recall: 0.7016\n",
      "Epoch 27/50\n",
      "Train Loss: 0.8004, Train Accuracy: 0.7047, Train Precision: 0.6569, Train Recall: 0.7047\n",
      "Validation Loss: 0.8236, Validation Accuracy: 0.6752, Validation Precision: 0.6082, Validation Recall: 0.6752\n",
      "Test Accuracy: 0.7026, Test Precision: 0.6425, Test Recall: 0.7026\n",
      "Epoch 28/50\n",
      "Train Loss: 0.8000, Train Accuracy: 0.7060, Train Precision: 0.6584, Train Recall: 0.7060\n",
      "Validation Loss: 0.8229, Validation Accuracy: 0.6774, Validation Precision: 0.6119, Validation Recall: 0.6774\n",
      "Test Accuracy: 0.7026, Test Precision: 0.6429, Test Recall: 0.7026\n",
      "Epoch 29/50\n",
      "Train Loss: 0.7997, Train Accuracy: 0.7057, Train Precision: 0.6584, Train Recall: 0.7057\n",
      "Validation Loss: 0.8224, Validation Accuracy: 0.6752, Validation Precision: 0.6086, Validation Recall: 0.6752\n",
      "Test Accuracy: 0.7026, Test Precision: 0.6427, Test Recall: 0.7026\n",
      "Epoch 30/50\n",
      "Train Loss: 0.7986, Train Accuracy: 0.7069, Train Precision: 0.6612, Train Recall: 0.7069\n",
      "Validation Loss: 0.8219, Validation Accuracy: 0.6763, Validation Precision: 0.6101, Validation Recall: 0.6763\n",
      "Test Accuracy: 0.7016, Test Precision: 0.6412, Test Recall: 0.7016\n",
      "Epoch 31/50\n",
      "Train Loss: 0.7987, Train Accuracy: 0.7057, Train Precision: 0.6580, Train Recall: 0.7057\n",
      "Validation Loss: 0.8214, Validation Accuracy: 0.6763, Validation Precision: 0.6108, Validation Recall: 0.6763\n",
      "Test Accuracy: 0.7026, Test Precision: 0.6429, Test Recall: 0.7026\n",
      "Epoch 32/50\n",
      "Train Loss: 0.7981, Train Accuracy: 0.7064, Train Precision: 0.6606, Train Recall: 0.7064\n",
      "Validation Loss: 0.8210, Validation Accuracy: 0.6774, Validation Precision: 0.6118, Validation Recall: 0.6774\n",
      "Test Accuracy: 0.7026, Test Precision: 0.6429, Test Recall: 0.7026\n",
      "Epoch 33/50\n",
      "Train Loss: 0.7967, Train Accuracy: 0.7066, Train Precision: 0.6598, Train Recall: 0.7066\n",
      "Validation Loss: 0.8205, Validation Accuracy: 0.6763, Validation Precision: 0.6106, Validation Recall: 0.6763\n",
      "Test Accuracy: 0.7026, Test Precision: 0.6429, Test Recall: 0.7026\n",
      "Epoch 34/50\n",
      "Train Loss: 0.7984, Train Accuracy: 0.7061, Train Precision: 0.6595, Train Recall: 0.7061\n",
      "Validation Loss: 0.8200, Validation Accuracy: 0.6763, Validation Precision: 0.6106, Validation Recall: 0.6763\n",
      "Test Accuracy: 0.7026, Test Precision: 0.6429, Test Recall: 0.7026\n",
      "Epoch 35/50\n",
      "Train Loss: 0.7964, Train Accuracy: 0.7066, Train Precision: 0.6600, Train Recall: 0.7066\n",
      "Validation Loss: 0.8195, Validation Accuracy: 0.6752, Validation Precision: 0.6082, Validation Recall: 0.6752\n",
      "Test Accuracy: 0.7036, Test Precision: 0.6443, Test Recall: 0.7036\n",
      "Epoch 36/50\n",
      "Train Loss: 0.7962, Train Accuracy: 0.7064, Train Precision: 0.6603, Train Recall: 0.7064\n",
      "Validation Loss: 0.8191, Validation Accuracy: 0.6774, Validation Precision: 0.6119, Validation Recall: 0.6774\n",
      "Test Accuracy: 0.7046, Test Precision: 0.6455, Test Recall: 0.7046\n",
      "Epoch 37/50\n",
      "Train Loss: 0.7949, Train Accuracy: 0.7074, Train Precision: 0.6622, Train Recall: 0.7074\n",
      "Validation Loss: 0.8186, Validation Accuracy: 0.6774, Validation Precision: 0.6127, Validation Recall: 0.6774\n",
      "Test Accuracy: 0.7026, Test Precision: 0.6424, Test Recall: 0.7026\n",
      "Epoch 38/50\n",
      "Train Loss: 0.7957, Train Accuracy: 0.7074, Train Precision: 0.6621, Train Recall: 0.7074\n",
      "Validation Loss: 0.8183, Validation Accuracy: 0.6774, Validation Precision: 0.6121, Validation Recall: 0.6774\n",
      "Test Accuracy: 0.7036, Test Precision: 0.6437, Test Recall: 0.7036\n",
      "Epoch 39/50\n",
      "Train Loss: 0.7950, Train Accuracy: 0.7077, Train Precision: 0.6632, Train Recall: 0.7077\n",
      "Validation Loss: 0.8180, Validation Accuracy: 0.6785, Validation Precision: 0.6140, Validation Recall: 0.6785\n",
      "Test Accuracy: 0.7046, Test Precision: 0.6455, Test Recall: 0.7046\n",
      "Epoch 40/50\n",
      "Train Loss: 0.7942, Train Accuracy: 0.7074, Train Precision: 0.6624, Train Recall: 0.7074\n",
      "Validation Loss: 0.8173, Validation Accuracy: 0.6774, Validation Precision: 0.6121, Validation Recall: 0.6774\n",
      "Test Accuracy: 0.7036, Test Precision: 0.6439, Test Recall: 0.7036\n",
      "Epoch 41/50\n",
      "Train Loss: 0.7937, Train Accuracy: 0.7077, Train Precision: 0.6636, Train Recall: 0.7077\n",
      "Validation Loss: 0.8168, Validation Accuracy: 0.6774, Validation Precision: 0.6121, Validation Recall: 0.6774\n",
      "Test Accuracy: 0.7026, Test Precision: 0.6420, Test Recall: 0.7026\n",
      "Epoch 42/50\n",
      "Train Loss: 0.7932, Train Accuracy: 0.7076, Train Precision: 0.6622, Train Recall: 0.7076\n",
      "Validation Loss: 0.8165, Validation Accuracy: 0.6796, Validation Precision: 0.6164, Validation Recall: 0.6796\n",
      "Test Accuracy: 0.7036, Test Precision: 0.6438, Test Recall: 0.7036\n",
      "Epoch 43/50\n",
      "Train Loss: 0.7933, Train Accuracy: 0.7078, Train Precision: 0.6624, Train Recall: 0.7078\n",
      "Validation Loss: 0.8160, Validation Accuracy: 0.6785, Validation Precision: 0.6146, Validation Recall: 0.6785\n",
      "Test Accuracy: 0.7056, Test Precision: 0.6476, Test Recall: 0.7056\n",
      "Epoch 44/50\n",
      "Train Loss: 0.7937, Train Accuracy: 0.7076, Train Precision: 0.6637, Train Recall: 0.7076\n",
      "Validation Loss: 0.8155, Validation Accuracy: 0.6796, Validation Precision: 0.6207, Validation Recall: 0.6796\n",
      "Test Accuracy: 0.7066, Test Precision: 0.6487, Test Recall: 0.7066\n",
      "Epoch 45/50\n",
      "Train Loss: 0.7918, Train Accuracy: 0.7088, Train Precision: 0.6648, Train Recall: 0.7088\n",
      "Validation Loss: 0.8151, Validation Accuracy: 0.6785, Validation Precision: 0.6201, Validation Recall: 0.6785\n",
      "Test Accuracy: 0.7076, Test Precision: 0.6588, Test Recall: 0.7076\n",
      "Epoch 46/50\n",
      "Train Loss: 0.7920, Train Accuracy: 0.7077, Train Precision: 0.6636, Train Recall: 0.7077\n",
      "Validation Loss: 0.8149, Validation Accuracy: 0.6763, Validation Precision: 0.6148, Validation Recall: 0.6763\n",
      "Test Accuracy: 0.7066, Test Precision: 0.6570, Test Recall: 0.7066\n",
      "Epoch 47/50\n",
      "Train Loss: 0.7909, Train Accuracy: 0.7079, Train Precision: 0.6627, Train Recall: 0.7079\n",
      "Validation Loss: 0.8142, Validation Accuracy: 0.6774, Validation Precision: 0.6186, Validation Recall: 0.6774\n",
      "Test Accuracy: 0.7056, Test Precision: 0.6592, Test Recall: 0.7056\n",
      "Epoch 48/50\n",
      "Train Loss: 0.7912, Train Accuracy: 0.7084, Train Precision: 0.6645, Train Recall: 0.7084\n",
      "Validation Loss: 0.8139, Validation Accuracy: 0.6774, Validation Precision: 0.6186, Validation Recall: 0.6774\n",
      "Test Accuracy: 0.7056, Test Precision: 0.6561, Test Recall: 0.7056\n",
      "Epoch 49/50\n",
      "Train Loss: 0.7908, Train Accuracy: 0.7073, Train Precision: 0.6624, Train Recall: 0.7073\n",
      "Validation Loss: 0.8135, Validation Accuracy: 0.6796, Validation Precision: 0.6223, Validation Recall: 0.6796\n",
      "Test Accuracy: 0.7056, Test Precision: 0.6555, Test Recall: 0.7056\n",
      "Epoch 50/50\n",
      "Train Loss: 0.7892, Train Accuracy: 0.7076, Train Precision: 0.6627, Train Recall: 0.7076\n",
      "Validation Loss: 0.8132, Validation Accuracy: 0.6785, Validation Precision: 0.6220, Validation Recall: 0.6785\n",
      "Test Accuracy: 0.7076, Test Precision: 0.6642, Test Recall: 0.7076\n",
      "Final test accuracy: 0.7056\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "\n",
    "best_val_acc = test_acc = 0\n",
    "times = []\n",
    "epochs = 50 \n",
    "best_epoch = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train() \n",
    "    total_train_loss = 0\n",
    "    all_train_labels = []\n",
    "    all_train_preds = []\n",
    "\n",
    "    # training phase\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        out = model(data.x, data.edge_index, data.edge_attr, data.batch)\n",
    "        \n",
    "        loss = F.cross_entropy(out, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_train_loss += loss.item()\n",
    "        \n",
    "        pred = out.argmax(dim=1)\n",
    "        all_train_labels.append(data.y.cpu().numpy())\n",
    "        all_train_preds.append(pred.cpu().numpy())\n",
    "\n",
    "    all_train_labels = np.concatenate(all_train_labels)\n",
    "    all_train_preds = np.concatenate(all_train_preds)\n",
    "    train_accuracy = accuracy_score(all_train_labels, all_train_preds)\n",
    "    train_precision = precision_score(all_train_labels, all_train_preds, average='weighted', zero_division=1)\n",
    "    train_recall = recall_score(all_train_labels, all_train_preds, average='weighted', zero_division=1)\n",
    "\n",
    "    # validation phase\n",
    "    model.eval() \n",
    "    total_val_loss = 0\n",
    "    all_val_labels = []\n",
    "    all_val_preds = []\n",
    "    with torch.no_grad():\n",
    "        for data in val_loader:\n",
    "            data = data.to(device)\n",
    "            out = model(data.x, data.edge_index, data.edge_attr, data.batch)\n",
    "            val_loss = F.cross_entropy(out, data.y)\n",
    "            total_val_loss += val_loss.item()\n",
    "\n",
    "            pred = out.argmax(dim=1)\n",
    "            all_val_labels.append(data.y.cpu().numpy())\n",
    "            all_val_preds.append(pred.cpu().numpy())\n",
    "\n",
    "    val_loss = total_val_loss / len(val_loader)\n",
    "    \n",
    "    all_val_labels = np.concatenate(all_val_labels)\n",
    "    all_val_preds = np.concatenate(all_val_preds)\n",
    "    val_accuracy = accuracy_score(all_val_labels, all_val_preds)\n",
    "    val_precision = precision_score(all_val_labels, all_val_preds, average='weighted', zero_division=1)\n",
    "    val_recall = recall_score(all_val_labels, all_val_preds, average='weighted', zero_division=1)\n",
    "\n",
    "    # updating learning rate based on validation loss\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    # test phase\n",
    "    model.eval()  \n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.linear = torch.nn.Linear(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr=None, batch=None):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        if batch is not None:\n",
    "            x = global_mean_pool(x, batch)\n",
    "\n",
    "        out = self.linear(x)\n",
    "        return out\n",
    "\n",
    "        class ImprovedGCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super(ImprovedGCN, self).__init__()\n",
    "    all_test_labels = []\n",
    "    all_test_preds = []\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            data = data.to(device)\n",
    "            out = model(data.x, data.edge_index, data.edge_attr, data.batch)\n",
    "\n",
    "            pred = out.argmax(dim=1)\n",
    "            all_test_labels.append(data.y.cpu().numpy())\n",
    "            all_test_preds.append(pred.cpu().numpy())\n",
    "\n",
    "    all_test_labels = np.concatenate(all_test_labels)\n",
    "    all_test_preds = np.concatenate(all_test_preds)\n",
    "    test_accuracy = accuracy_score(all_test_labels, all_test_preds)\n",
    "    test_precision = precision_score(all_test_labels, all_test_preds, average='weighted', zero_division=1)\n",
    "    test_recall = recall_score(all_test_labels, all_test_preds, average='weighted', zero_division=1)\n",
    "\n",
    "    # performance metrics\n",
    "    print(f'Epoch {epoch+1}/{epochs}')\n",
    "    print(f'Train Loss: {total_train_loss / len(train_loader):.4f}, Train Accuracy: {train_accuracy:.4f}, Train Precision: {train_precision:.4f}, Train Recall: {train_recall:.4f}')\n",
    "    print(f'Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}, Validation Precision: {val_precision:.4f}, Validation Recall: {val_recall:.4f}')\n",
    "    print(f'Test Accuracy: {test_accuracy:.4f}, Test Precision: {test_precision:.4f}, Test Recall: {test_recall:.4f}')\n",
    "\n",
    "    if val_accuracy > best_val_acc:\n",
    "        best_val_acc = val_accuracy\n",
    "        test_acc = test_accuracy\n",
    "        best_epoch = epoch\n",
    "\n",
    "print(f'Final test accuracy: {test_acc:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'GCN.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
